[["index.html", "Examples from Doing Bayesian Data Analysis Preface", " Examples from Doing Bayesian Data Analysis Melissa Wong 2021-02-28 Preface I picked up a used copy of the 1st edition of Doing Bayesian Data Analysis, so I created this project to keep a record of my notes as I work through the examples. "],["book-organization.html", "Chapter 1 Book Organization", " Chapter 1 Book Organization Introduction to the book. No exercises or examples. "],["introduction.html", "Chapter 2 Introduction 2.1 Exercise 2.1 2.2 Exercise 2.2 2.3 Exercise 2.3 2.4 Exercise 2.4 2.5 Excerise 2.5 2.6 Exercise 2.6", " Chapter 2 Introduction Concept of combining prior and posterior beliefs Three goals of inference: Parameter Estimation Prediction Model Comparison Introduction to R 2.1 Exercise 2.1 Since there is no way to see or feel the angels, there is no data which can be collected that can alter the belief. Conversely, it is possible to measure the number of dancing anglers and this is data which could be used to alter the belief. 2.2 Exercise 2.2 x p(x)=1/4 p(x)=x/10 p(x)=25/2x 1 0.25 0.1 0.48 2 0.25 0.2 0.24 3 0.25 0.3 0.16 4 0.25 0.4 0.12 2.3 Exercise 2.3 After #1’s=#2’s=#3’s=#4’s=25, Model A is more likely. After #1’s=48, #2’s=24, #3’s=16 and #4’s=12, Model C is more likely. 2.4 Exercise 2.4 Install R. 2.5 Excerise 2.5 x &lt;- seq(-2, 2, 0.1) y = x^2 ggplot()+ geom_line(mapping=aes(x=x, y=y)) 2.6 Exercise 2.6 x &lt;- seq(-3, 3, 0.1) y = x^3 ggplot()+ geom_line(mapping=aes(x=x, y=y)) "],["intro-to-probability.html", "Chapter 3 Intro to Probability 3.1 Exercise 3.1 3.2 Exercise 3.2 3.3 Exercise 3.3 3.4 Exercsie 3.4 3.5 Exercise 3.5", " Chapter 3 Intro to Probability 3.1 Exercise 3.1 library(tidyverse) set.seed(123) # Number of coin flips N &lt;- 500 p &lt;- 0.8 flips &lt;- sample(x=c(&quot;H&quot;, &quot;T&quot;), prob=c(p, 1-p), size=N, replace=TRUE) Phead &lt;- cumsum(flips==&quot;H&quot;) / (1:N) ggplot() + geom_line(mapping=aes(x=1:N, y=Phead), alpha=0.5) + geom_hline(mapping=aes(yintercept=p), color=&quot;red&quot;) 3.2 Exercise 3.2 A. \\(P[X=10] = \\frac{8}{48} = \\frac{1}{6}\\) B. \\(P[X=10 or X=Jack] = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}\\) 3.3 Exercise 3.3 dx &lt;- 0.01 x &lt;- seq(0, 1, dx) px &lt;- 6*x * (1-x) ggplot()+ geom_line(mapping=aes(x=x, y=px)) + labs(title=&quot;pdf&quot;) # Approximate Integral area = sum(dx * px) paste(&quot;The area under the curve is approximately&quot;, area) ## [1] &quot;The area under the curve is approximately 0.9999&quot; \\[\\begin{align*} \\int_0^1 6x(1-x) dx &amp;= \\int_0^1 6xdx - \\int_0^1 6x^2dx \\\\ &amp;= 3x^2 \\rvert_0^1 - 2x^3 \\rvert_0^1 \\\\ &amp;= 3 - 2 \\\\ &amp;= 1 \\end{align*}\\] C. Yes, it satisfies equation 3.3. D. The maximum value of $p(x) = 1.5 3.4 Exercsie 3.4 A. and B. mu &lt;- 162 sigma &lt;- 15 dx &lt;- 0.01 x &lt;- seq(mu-sigma, mu+sigma, dx) px &lt;- 1/(sigma * sqrt(2*pi)) * exp(-((x-mu)/sigma)^2/2) ggplot()+ geom_line(mapping=aes(x=x, y=px)) + labs(title=&quot;pdf&quot;) # Approximate Integral area = sum(dx * px) paste(&quot;The area under the curve is approximately&quot;, round(area, 2)) ## [1] &quot;The area under the curve is approximately 0.68&quot; 3.5 Exercise 3.5 \\(P[grade=1st] = 0.2, P[grade=6th]=0.2, P[grade=11th]=0.6\\) Conjoint Table Grade Ice Cream Fruit French Fries 1st 0.06 0.12 0.02 6th 0.12 0.06 0.02 11th 0.18 0.06 0.36 Grade and favorite food are independent if \\(P[\\text{favorite food | grade}] = P[\\text{favorite food}]\\) \\[P[\\text{ice cream} | 1st] = 0.3 \\ne P[\\text{ice cream}] = 0.36\\] \\(\\therefore\\) Grade and favorite food are not independent. "],["bayes-rule.html", "Chapter 4 Bayes’ Rule 4.1 Exercise 4.1 4.2 Exercise 4.2 4.3 Exercise 4.3 4.4 Exercise 4.4 4.5 Exercise 4.5 4.6 Exercise 4.6 4.7 Exercise 4.7", " Chapter 4 Bayes’ Rule 4.1 Exercise 4.1 Given \\(P(\\theta = disease) = 0.001\\), \\(P(+ | disease) = 0.99\\) and \\(P(+ | no disease) = 0.05\\) \\[\\begin{align*} P(disease | +) &amp;= \\frac{0.99 * 0.001}{0.99 * 0.001 + 0.05 * 0.999} \\\\ &amp;= 0.019 \\end{align*}\\] 4.2 Exercise 4.2 \\[\\begin{align*} P(disease | -,+) &amp;= \\frac{P(- | disease) * 0.019)}{P(-)} \\\\ &amp;= \\frac{0.01 * 0.019}{0.01 * 0.001 + 0.95 * 0.999} \\\\ &amp;= 0.0002 \\end{align*}\\] 4.3 Exercise 4.3 4.3.1 Part a disease no disease D=+ 99 4995 D=- 1 94905 100 99900 4.3.2 Part b \\(P(disease | +) = \\frac{99}{99 + 4995} = 0.019\\) 4.3.3 Part c 1st stage: Number of people with disease who test postive: 10000 * 0.99 = 9900 Number of people without disease who test positive: 9990000 * 0.05 = 499500 2nd stage - Retest those with positive tests Number of people with disease who test negative 2nd time: 9900 * 0.01 = 99 Number of people without disease who test negative 2nd time: 499500 * 0.95 = 474525 4.3.4 Part d \\(P(disease | -, +) = \\frac{99}{99 + 474525} = 0.0002\\) 4.4 Exercise 4.4 \\[\\begin{align*} P(D | -) &amp;= \\frac{P(-|D)P(D)}{P(-)} \\\\ &amp;= \\frac{0.01 * 0.001}{0.01 * 0.001 + 0.95 * 0.999} \\\\ &amp;= 1.054* 10^{-5} \\end{align*}\\] \\[\\begin{align*} P(D | +, -) &amp;= \\frac{P(+|D) * 1.054*10^{-5}}{P(+)} \\\\ &amp;= \\frac{0.99 * 1.054*10^{-5}}{0.99*0.001 + 0.05 * 0.999} \\\\ &amp;= 0.0002 \\end{align*}\\] The result is the same as Part d. In other words, the order of the tests doesn’t change the conclusion which is consistent with the assumption that the tests are independent. 4.5 Exercise 4.5 Given \\(P(Language Study) = 0.5\\), then \\[\\begin{align*} P(Language Study | ROI) &amp;= \\frac{P(ROI|Language Study) P(Language Study)}{P(ROI)}\\\\ &amp;= \\frac{166/(166+703) * 0.5}{0.5*(166/(166+703) + 199/(199+2154))} \\\\ &amp;= 0.69 \\end{align*}\\] 4.6 Exercise 4.6 theta &lt;- c(0.25, 0.5, 0.75) prior &lt;- c(0.25, 0.5, 0.25) likelihood &lt;- theta^3 * (1-theta)^9 num &lt;- prior * likelihood den &lt;- sum(num) post &lt;- num/den knitr::kable(tibble::tibble(theta = theta, posterior = post)) theta posterior 0.25 0.7054333 0.50 0.2935990 0.75 0.0009677 4.7 Exercise 4.7 \\(P(D) = P(D|\\theta=0.25)*0.25 + P(D|\\theta=0.5) * 0.5 + P(D|\\theta=0.75) * 0.25\\) which is the same as the sum of the likelihood * prior calculated in exercise 4.6. "],["binomial-proportions-via-mathematical-analysis.html", "Chapter 5 Binomial Proportions via Mathematical Analysis 5.1 Excercise 5.1 5.2 Excercse 5.3 5.3 Exercise 5.4 5.4 Exercise 5.5 5.5 Exercise 5.6 5.6 Exercise 5.7 5.7 Exercise 5.8", " Chapter 5 Binomial Proportions via Mathematical Analysis library(tidyr) library(magrittr) library(ggplot2) library(RColorBrewer) 5.1 Excercise 5.1 # HHT Sequence N &lt;- 100 x &lt;- seq(0, 1, length.out = N) prior &lt;- dbeta(x, 4, 4) post1 &lt;- dbeta(x, 5, 4) post2 &lt;- dbeta(x, 6, 4) post3 &lt;- dbeta(x, 6, 5) tibble::tibble(x = x, prior = prior, post1 = post1, post2 = post2, post3 = post3) %&gt;% pivot_longer(cols=-x, values_to=&quot;density&quot;, names_to=&quot;type&quot;) %&gt;% ggplot() + geom_line(mapping=aes(x=x, y=density, color=type)) + scale_color_brewer(palette=&quot;Set2&quot;) # THH Sequence N &lt;- 100 x &lt;- seq(0, 1, length.out = N) prior &lt;- dbeta(x, 4, 4) post1 &lt;- dbeta(x, 4, 5) post2 &lt;- dbeta(x, 5, 5) post3 &lt;- dbeta(x, 6, 5) tibble::tibble(x = x, prior = prior, post1 = post1, post2 = post2, post3 = post3) %&gt;% pivot_longer(cols=-x, values_to=&quot;density&quot;, names_to=&quot;type&quot;) %&gt;% ggplot() + geom_line(mapping=aes(x=x, y=density, color=type)) + scale_color_brewer(palette=&quot;Set2&quot;) ## Exercise 5.2 5.1.1 Part a HDIofICDF = function( ICDFname , credMass=0.95 , tol=1e-8 , ... ) { # Arguments: # ICDFname is R&#39;s name for the inverse cumulative density function # of the distribution. # credMass is the desired mass of the HDI region. # tol is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30,12) distribution, type # HDIofICDF( qbeta , shape1 = 30 , shape2 = 12 ) # Notice that the parameters of the ICDFname must be explicitly named; # e.g., HDIofICDF( qbeta , 30 , 12 ) does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredMass = 1.0 - credMass intervalWidth = function( lowTailPr , ICDFname , credMass , ... ) { ICDFname( credMass + lowTailPr , ... ) - ICDFname( lowTailPr , ... ) } optInfo = optimize( intervalWidth , c( 0 , incredMass ) , ICDFname=ICDFname , credMass=credMass , tol=tol , ... ) HDIlowTailPr = optInfo$minimum return( c( ICDFname( HDIlowTailPr , ... ) , ICDFname( credMass + HDIlowTailPr , ... ) ) ) } (lim &lt;- HDIofICDF(qbeta, shape1=59, shape2=43)) ## [1] 0.4828696 0.6731385 x &lt;- seq(0, 1, length.out = 100) ggplot() + geom_line(mapping=aes(x = x, y = dbeta(x, 59, 43))) + geom_vline(mapping=aes(xintercept=lim), linetype=&quot;dashed&quot;) ### Part b Yes, it is credible to believe the population is evenly split between the two candidates (0.5 is within the 95% credible interval). 5.1.2 Part c HDIofICDF(qbeta, shape1=57+59, shape2=43+43) ## [1] 0.5061148 0.6419895 5.1.3 Part d No, it is not credible to believe the population is evenly split between the two candidates (0.5 is outside the 95% credible interval). 5.2 Excercse 5.3 HDIofICDF(qbeta, shape1=41, shape2=11) ## [1] 0.6771853 0.8934740 HDIofICDF(qbeta, shape1=16, shape2=36) ## [1] 0.1865395 0.4329478 In the first case, subjects are biased towards “F”, and in the second case subjects are biased towards “J”. 5.3 Exercise 5.4 (lim &lt;- HDIofICDF(qbeta, shape1=4.5, shape2=1.5)) ## [1] 0.4360173 0.9982900 x &lt;- seq(0, 1, length.out = 100) ggplot() + geom_line(mapping=aes(x = x, y = dbeta(x, 4.5, 1.5))) + geom_vline(mapping=aes(xintercept=lim), linetype=&quot;dashed&quot;) 5.4 Exercise 5.5 5.4.1 Part a # HHT Sequence N &lt;- 100 x &lt;- seq(0, 1, length.out = N) prior &lt;- dbeta(x, 100, 100) post1 &lt;- dbeta(x, 109, 101) tibble::tibble(x = x, prior = prior, post1 = post1) %&gt;% pivot_longer(cols=-x, values_to=&quot;density&quot;, names_to=&quot;type&quot;) %&gt;% ggplot() + geom_line(mapping=aes(x=x, y=density, color=type)) + scale_color_brewer(palette=&quot;Set2&quot;) The predicted probability for the next coin flip is the expected value of Beta(109, 101) which is \\(\\frac{109}{109+101} = 0.519\\) 5.4.2 Part b # HHT Sequence N &lt;- 100 x &lt;- seq(0, 1, length.out = N) prior &lt;- dbeta(x, 0.5, 0.5) post1 &lt;- dbeta(x, 9.5, 1.5) tibble::tibble(x = x, prior = prior, post1 = post1) %&gt;% pivot_longer(cols=-x, values_to=&quot;density&quot;, names_to=&quot;type&quot;) %&gt;% ggplot() + geom_line(mapping=aes(x=x, y=density, color=type)) + scale_color_brewer(palette=&quot;Set2&quot;) The predicted probability for the next coin flip is the expected value of Beta(9.5, 1.5) which is \\(\\frac{9.5}{9.5+1.5} = 0.864\\) 5.5 Exercise 5.6 N &lt;- 100 x &lt;- seq(0, 1, length.out = N) # Model 1 - Fair coin post1 &lt;- dbeta(x, 105, 115) prob1 &lt;- beta(115, 105)/beta(100, 100) # Model 2 - Biased coin post2 &lt;- dbeta(x, 15.5, 5.5) prob2 &lt;- beta(15.5, 5.5)/beta(0.5, 0.5) tibble::tibble(x = x, post1 = post1, post2 = post2) %&gt;% pivot_longer(cols=-x, values_to=&quot;density&quot;, names_to=&quot;type&quot;) %&gt;% ggplot() + geom_line(mapping=aes(x=x, y=density, color=type)) + scale_color_brewer(palette=&quot;Set2&quot;) Fair coin model probability = 1.142463110^{-6}. Biased coin model probability = 2.293072610^{-6}. The Bayes Factor = 0.4982237. There is slightly more evidence in favor of the biased coin model. 5.6 Exercise 5.7 N &lt;- 100 x &lt;- seq(0, 1, length.out = N) # Model 1 - Tail-biased coin post1 &lt;- dbeta(x, 2, 100) prob1 &lt;- beta(2, 100)/beta(1, 100) # Model 2 - Head-biased coin post2 &lt;- dbeta(x, 101, 1) prob2 &lt;- beta(101, 1)/beta(100, 1) tibble::tibble(x = x, post1 = post1, post2 = post2) %&gt;% pivot_longer(cols=-x, values_to=&quot;density&quot;, names_to=&quot;type&quot;) %&gt;% ggplot() + geom_line(mapping=aes(x=x, y=density, color=type)) + scale_color_brewer(palette=&quot;Set2&quot;) Tail-biased coin model probability = 0.009901. Head-biased model probability = 0.990099. The Bayes Factor = 0.01. There is significantly more evidence in favor of the head-biased coin model. 5.7 Exercise 5.8 5.7.1 Part a N &lt;- 100 x &lt;- seq(0, 1, length.out = N) # Model 1 - Tail-biased coin post1 &lt;- dbeta(x, 9, 104) prob1 &lt;- beta(9, 104)/beta(1, 100) # Model 2 - Head-biased coin post2 &lt;- dbeta(x, 108, 5) prob2 &lt;- beta(108, 5)/beta(100, 1) tibble::tibble(x = x, post1 = post1, post2 = post2) %&gt;% pivot_longer(cols=-x, values_to=&quot;density&quot;, names_to=&quot;type&quot;) %&gt;% ggplot() + geom_line(mapping=aes(x=x, y=density, color=type)) + scale_color_brewer(palette=&quot;Set2&quot;) Tail-biased coin model probability = 2.022200910^{-12}. Head-biased model probability = 1.490827210^{-7}. The Bayes Factor = 1.356428810^{-5}. There is significantly more evidence in favor of the head-biased coin model. 5.7.2 Part b iterations &lt;- 1000 heads &lt;- rep(NA, iterations) for (i in seq_along(heads)) { theta &lt;- rbeta(1, 108, 5) heads[i] &lt;- rbinom(1, 12, theta) } ggplot()+ geom_histogram(mapping=aes(x=heads)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 1000 samples of size 12 were simulated. 5.7.3 Part c Different values of \\(\\theta\\) were used for each simulated sample because each time we sample from the posterior distribution of \\(\\theta\\). 5.7.4 Part d The heads-biased model, while better than the tails-biased model, is still not a good fit to the observed data (8 heads in 12 tosses); it predicts a much higher number of heads than observed. "],["references.html", "References", " References "]]
